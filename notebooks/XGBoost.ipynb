{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing, load_digits, load_iris\n",
    "from sklearn.metrics import confusion_matrix, mean_squared_error\n",
    "from sklearn.model_selection import GridSearchCV, KFold, train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "\n",
    "# df = pd.read_csv('entities-2023-09-02_17 33 41.csv', usecols=['start', 'mean'])\n",
    "df = pd.read_csv('entities-2023-09-06_10 17 37.csv', usecols=['start', 'mean'])\n",
    "df['start'] = pd.to_datetime(df['start'], format='%Y-%m-%d %H:%M:%S')\n",
    "df = df.set_index('start')\n",
    "df = df.asfreq(freq='H')\n",
    "df = df[['mean']]\n",
    "df = df.tail(10*7*24)\n",
    "print(len(df) / 24)\n",
    "\n",
    "# df['holiday'] = False\n",
    "# df.loc['2023-08-25 00:00:00':'2023-09-03 00:00:00', 'holiday'] = True\n",
    "\n",
    "df['hour'] = df.index.hour\n",
    "df['dayofweek'] = df.index.dayofweek\n",
    "df['is_weekend'] = (df.index.dayofweek >= 5)\n",
    "df['is_weekend'] = df['is_weekend'].replace({True: 1, False: 0})\n",
    "# df['day_so_far'] = df.groupby(df.index.day)['mean'].cumsum()\n",
    "df['past_24h'] = df['mean'].rolling(24).sum().shift()\n",
    "df['yesterday'] = df.groupby(df.index.date)['mean'].sum().shift(1) / 24\n",
    "df['yesterday'] = df['yesterday'].fillna(method='ffill')\n",
    "df['yesterday_2'] = df.groupby(df.index.date)['mean'].sum().shift(2) / 24\n",
    "df['yesterday_2'] = df['yesterday_2'].fillna(method='ffill')\n",
    "df['yesterday_peak'] = df.groupby(df.index.date)['mean'].max().shift(1)\n",
    "df['yesterday_peak'] = df['yesterday_peak'].fillna(method='ffill')\n",
    "df['hlag_1d'] = df['mean'].shift(24)\n",
    "df['hlag_2d'] = df['mean'].shift(24*2)\n",
    "df['hlag_3d'] = df['mean'].shift(24*3)\n",
    "df['hlag_4d'] = df['mean'].shift(24*4)\n",
    "df['hlag_5d'] = df['mean'].shift(24*5)\n",
    "df['hlag_6d'] = df['mean'].shift(24*6)\n",
    "df['hlag_7d'] = df['mean'].shift(24*7)\n",
    "df['yesterday_25'] =  df.groupby(df.index.date)['mean'].quantile(0.8).shift(1) / 24\n",
    "df['yesterday_25'] = df['yesterday_25'].fillna(method='ffill')\n",
    "\n",
    "df['cum_sum_today'] = df.groupby(df.index.date)['mean'].cumsum().shift(1)\n",
    "df['lag_1h'] = df['mean'].shift(1)\n",
    "df['lag_2h'] = df['mean'].shift(2)\n",
    "df['lag_3h'] = df['mean'].shift(3)\n",
    "\n",
    "df['sum_today'] = df.groupby(df.index.date)['mean'].sum()\n",
    "df['sum_today'] = df['sum_today'].fillna(method='ffill')\n",
    "df['mean_today'] = df['sum_today'] / 24\n",
    "\n",
    "# stl = \"\"\"{\"predicted\":{\"1689811200000\":6.1024130505,\"1689897600000\":5.2882367582,\"1689984000000\":7.1046379757,\"1690070400000\":7.5558324369,\"1690156800000\":7.4207686308,\"1690243200000\":7.1760769877,\"1690329600000\":5.8848356135,\"1690416000000\":7.0620552752,\"1690502400000\":5.3280016617,\"1690588800000\":8.6262117886,\"1690675200000\":8.2011682559,\"1690761600000\":9.6541242131,\"1690848000000\":6.395082486,\"1690934400000\":6.2387428456,\"1691020800000\":6.0210112227,\"1691107200000\":5.9131599036,\"1691193600000\":6.8258694141,\"1691280000000\":6.067949583,\"1691366400000\":5.2549148829,\"1691452800000\":6.3373241805,\"1691539200000\":5.5160181919,\"1691625600000\":7.5489618675,\"1691712000000\":6.1272265017,\"1691798400000\":7.850609946,\"1691884800000\":8.1679921251,\"1691971200000\":8.8346228733,\"1692057600000\":7.1282213566,\"1692144000000\":6.6773378019,\"1692230400000\":6.6954823189,\"1692316800000\":7.0140371574,\"1692403200000\":7.5789723455,\"1692489600000\":6.9975712542,\"1692576000000\":7.1835653678,\"1692662400000\":6.4692300345,\"1692748800000\":6.2646061006,\"1692835200000\":6.5575620771,\"1692921600000\":7.4596137307,\"1693008000000\":6.8102031764,\"1693094400000\":6.0367357611,\"1693180800000\":4.9065749756,\"1693267200000\":4.1936456354,\"1693353600000\":4.334267945,\"1693440000000\":4.7481844442,\"1693526400000\":4.5254548537,\"1693612800000\":4.4046905524,\"1693699200000\":5.1102698367,\"1693785600000\":10.1143400729,\"1693872000000\":11.3499860392}}\"\"\"\n",
    "# stl = pd.read_json(stl).rename(columns={\"predicted\": \"stl_sum_today\"}).asfreq('D')\n",
    "# df = stl.combine_first(df)\n",
    "# df['stl_sum_today'] = df['stl_sum_today'].fillna(method='ffill')\n",
    "# df['stl_sum_today'] = df['stl_sum_today'].fillna(df['sum_today'])\n",
    "\n",
    "exogs = ['hour', 'dayofweek', 'yesterday', 'yesterday_2', 'yesterday_peak','hlag_1d', 'hlag_2d', 'hlag_3d','hlag_4d','hlag_5d','hlag_6d', 'hlag_7d']\n",
    "# exogs = ['hour', 'dayofweek', 'yesterday', 'yesterday_2', 'yesterday_peak', 'hlag_1d', 'hlag_2d', 'hlag_3d', 'hlag_7d']\n",
    "# exogs = ['hour', 'dayofweek', 'yesterday', 'cum_sum_today', 'lag_1h', 'lag_2h', 'lag_3h']\n",
    "\n",
    "# train_period = 24 * 7 * 9\n",
    "train_period = 24*7*5\n",
    "test_period = 24*7\n",
    "# # for x in range(7*3):\n",
    "for x in range(0, len(df) - train_period - test_period, 24):\n",
    "    train_start = x\n",
    "    train_end = x + train_period\n",
    "    test_start = train_end\n",
    "    test_end = test_start + test_period\n",
    "    predict_start = test_end\n",
    "    predict_end = predict_start + 24\n",
    "    predict_start_date = df.iloc[predict_start].name\n",
    "\n",
    "    print(f\"Training from {train_start} - {train_end - 1}: {df.iloc[train_start].name} - {df.iloc[train_end - 1].name}\")\n",
    "\n",
    "    X_train = df[train_start:train_end][exogs]\n",
    "    y_train = df[train_start:train_end]['mean']\n",
    "\n",
    "    X_test = df[test_start:test_end][exogs]\n",
    "    y_test = df[test_start:test_end]['mean']\n",
    "    \n",
    "    X_predict = df[predict_start:predict_end][exogs]\n",
    "\n",
    "    params = {'max_depth': 6, 'min_child_weight': 2, 'eta': 0.3, 'subsample': 0.7, 'colsample_bytree': 0.7, 'objective': 'reg:squaredlogerror'}\n",
    "    # model = xgb.XGBRegressor(verbosity=0, num_estimators=500, early_stopping_rounds=100, objective='reg:squaredlogerror')\n",
    "    model = xgb.XGBRegressor(verbosity=0, **params)\n",
    "    model.fit(X_train, y_train, verbose=False, eval_set=[(X_train, y_train), (X_test, y_test)])\n",
    "\n",
    "    # xgb.plot_importance(model, height=0.9)\n",
    "\n",
    "    prediction = model.predict(X_predict)\n",
    "    forecasted_df = pd.DataFrame(prediction, columns=['predicted'], index=pd.date_range(predict_start_date, predict_start_date + pd.Timedelta(hours=23), freq='H'))\n",
    "    df = df.combine_first(forecasted_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from matplotlib import pyplot\n",
    "\n",
    "forecast_start = train_period + test_period\n",
    "\n",
    "# df = df[['mean', 'predicted']]\n",
    "print(df[forecast_start:][['mean', 'predicted']])\n",
    "\n",
    "# df['predicted_sum'] = df.groupby(df.index.date)['predicted'].sum()\n",
    "# df['predicted_sum'] = df['predicted_sum'].fillna(method='ffill')\n",
    "\n",
    "# df['predicted_norm'] = df['predicted'] * df['stl_sum_today'] / df['predicted_sum']\n",
    "\n",
    "\n",
    "\n",
    "daily_sums = df[forecast_start:].resample('D').sum()\n",
    "# daily_sums['stl_sum_today'] = daily_sums['stl_sum_today'] / 24\n",
    "\n",
    "pyplot.figure()\n",
    "fig, ax = pyplot.subplots(figsize=(15, 5))\n",
    "df[forecast_start-48:]['mean'].plot(ax=ax)\n",
    "df[forecast_start-48:]['predicted'].plot(ax=ax)\n",
    "# df[forecast_start-48:]['predicted_norm'].plot(ax=ax)\n",
    "# (daily_sums['mean'] - daily_sums['predicted']).shift(1).plot(ax=ax)\n",
    "# (df[forecast_start-48:]['mean'] - df[forecast_start-48:]['predicted']).plot(ax=ax)\n",
    "# ax.fill_between(df.index, df['predicted_lower'], df['predicted_upper'], color='k', alpha=0.1);  \n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "print(mean_absolute_error(df[forecast_start:]['mean'], df[forecast_start:]['predicted']))\n",
    "\n",
    "pyplot.figure()\n",
    "fig, ax = pyplot.subplots(figsize=(15, 5))\n",
    "daily_sums['mean'].plot(ax=ax)\n",
    "daily_sums['predicted'].plot(ax=ax)\n",
    "# daily_sums['predicted_norm'].plot(ax=ax)\n",
    "# daily_sums['stl_sum_today'].plot(ax=ax)\n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "\n",
    "# print(mean_absolute_error(df[forecast_start:]['mean'], df[forecast_start:]['predicted_norm']))\n",
    "print(mean_absolute_error(df[forecast_start:]['mean'], df[forecast_start:]['predicted']))\n",
    "# print(daily_sums[['mean', 'predicted', 'predicted_norm']])\n",
    "print(daily_sums[['mean', 'predicted']])\n",
    "# print(mean_absolute_error(daily_sums['mean'], daily_sums['predicted_norm']))\n",
    "# print(mean_squared_error(daily_sums['mean'], daily_sums['predicted_norm']))\n",
    "print(mean_absolute_error(daily_sums['mean'], daily_sums['predicted']))\n",
    "print(mean_squared_error(daily_sums['mean'], daily_sums['predicted']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://blog.cambridgespark.com/hyperparameter-tuning-in-xgboost-4ff9100a3b2f\n",
    "\n",
    "train_period = 7*10\n",
    "test_period = 7*2\n",
    "\n",
    "train_start = 0\n",
    "train_end = train_period\n",
    "\n",
    "X_train = df[train_start:train_end][exogs]\n",
    "y_train = df[train_start:train_end]['mean']\n",
    "\n",
    "# X_test = df[test_start:test_end][exogs]\n",
    "# y_test = df[test_start:test_end]['mean']\n",
    "\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':8,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squaredlogerror',\n",
    "}\n",
    "gridsearch_params = [\n",
    "    (max_depth, min_child_weight)\n",
    "    for max_depth in range(1,10)\n",
    "    for min_child_weight in range(1,10)\n",
    "]\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "for max_depth, min_child_weight in gridsearch_params:\n",
    "    print(\"CV with max_depth={}, min_child_weight={}\".format(\n",
    "                             max_depth,\n",
    "                             min_child_weight))    # Update our parameters\n",
    "    params['max_depth'] = max_depth\n",
    "    params['min_child_weight'] = min_child_weight    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=500,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=100\n",
    "    )    # Update best MAE\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (max_depth,min_child_weight)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['max_depth'] = 6\n",
    "params['min_child_weight'] = 2\n",
    "\n",
    "gridsearch_params = [\n",
    "    (subsample, colsample)\n",
    "    for subsample in [i/10. for i in range(7,11)]\n",
    "    for colsample in [i/10. for i in range(7,11)]\n",
    "]\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None# We start by the largest values and go down to the smallest\n",
    "for subsample, colsample in reversed(gridsearch_params):\n",
    "    print(\"CV with subsample={}, colsample={}\".format(\n",
    "                             subsample,\n",
    "                             colsample))    # We update our parameters\n",
    "    params['subsample'] = subsample\n",
    "    params['colsample_bytree'] = colsample    # Run CV\n",
    "    cv_results = xgb.cv(\n",
    "        params,\n",
    "        dtrain,\n",
    "        num_boost_round=500,\n",
    "        seed=42,\n",
    "        nfold=5,\n",
    "        metrics={'mae'},\n",
    "        early_stopping_rounds=100\n",
    "    )    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = (subsample,colsample)\n",
    "print(\"Best params: {}, {}, MAE: {}\".format(best_params[0], best_params[1], min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['subsample'] = 0.7\n",
    "params['colsample_bytree'] = 0.7\n",
    "\n",
    "min_mae = float(\"Inf\")\n",
    "best_params = None\n",
    "\n",
    "for eta in [.3, .2, .1, .05, .01, .005]:\n",
    "    print(\"CV with eta={}\".format(eta))    # We update our parameters\n",
    "    params['eta'] = eta    # Run and time CV\n",
    "    cv_results = xgb.cv(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=100,\n",
    "            seed=42,\n",
    "            nfold=5,\n",
    "            metrics=['mae'],\n",
    "            early_stopping_rounds=10\n",
    "          )    # Update best score\n",
    "    mean_mae = cv_results['test-mae-mean'].min()\n",
    "    boost_rounds = cv_results['test-mae-mean'].argmin()\n",
    "    print(\"\\tMAE {} for {} rounds\\n\".format(mean_mae, boost_rounds))\n",
    "    if mean_mae < min_mae:\n",
    "        min_mae = mean_mae\n",
    "        best_params = eta\n",
    "print(\"Best params: {}, MAE: {}\".format(best_params, min_mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params['eta'] = .3\n",
    "\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import LinearRegression\n",
    "from mlforecast import MLForecast\n",
    "from sklearn.ensemble import HistGradientBoostingRegressor, RandomForestRegressor\n",
    "from window_ops.rolling import rolling_mean, rolling_max, rolling_min\n",
    "\n",
    "df['unique_id'] = 'id'\n",
    "df['ds'] = df.index\n",
    "# df = df.drop(columns=['past_24h'])\n",
    "df['past_24h'] = df['mean'].rolling(24).sum().shift()\n",
    "input = df.copy()\n",
    "\n",
    "for x in range(0, len(df) - train_period, 24):\n",
    "# for x in range(1):\n",
    "    train_start = x\n",
    "    train_end = x + train_period\n",
    "    predict_start = input.iloc[train_end].name\n",
    "\n",
    "    print(f\"Training from {train_start} - {train_end - 1}: {input.iloc[train_start].name} - {input.iloc[train_end - 1].name}\")\n",
    "\n",
    "    # X_train = input[train_start:train_end][exogs]\n",
    "    y_train = input[train_start:train_end]['mean']\n",
    "\n",
    "    # X_test = input[train_end:train_end+24][exogs]\n",
    "    y_test = input[train_end:train_end+24]['mean']\n",
    "\n",
    "    models = [xgb.XGBRegressor(), HistGradientBoostingRegressor()]\n",
    "\n",
    "    model = MLForecast(models=models,\n",
    "                    freq='H',\n",
    "                    lags=[24,24*7],\n",
    "                    lag_transforms={\n",
    "                        2: [\n",
    "                            (rolling_mean, 24),\n",
    "                            (rolling_mean, 24*7),\n",
    "                        ]\n",
    "                    },\n",
    "                    date_features=['hour', 'dayofweek'],\n",
    "                    \n",
    "                    )\n",
    "\n",
    "    model.fit(input[train_start:train_end], id_col='unique_id', time_col='ds', target_col='mean', static_features=[])\n",
    "    print(input[train_end:train_end+24][['past_24h']])\n",
    "    pred = model.predict(horizon=24, dynamic_dfs=input[train_end:train_end+24][['past_24h']])\n",
    "    pred.index = pred['ds']\n",
    "    pred = pred[['XGBRegressor']]\n",
    "    pred = pred.rename(columns={'XGBRegressor': 'predicted'})\n",
    "\n",
    "    # model = xgb.XGBRegressor(verbosity=0, objective=\"reg:absoluteerror\", eval_metric=\"mae\")\n",
    "    # model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_test, y_test)], verbose=False)\n",
    "\n",
    "    # xgb.plot_importance(model, height=0.9)\n",
    "\n",
    "    # prediction = model.predict(X_test)\n",
    "    # forecasted_df = pd.DataFrame(prediction, columns=['predicted'], index=pd.date_range(predict_start, predict_start + pd.Timedelta(hours=23), freq='H'))\n",
    "    df = df.combine_first(pred)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from matplotlib import pyplot\n",
    "\n",
    "df = df[['mean', 'predicted']]\n",
    "\n",
    "pyplot.figure()\n",
    "fig, ax = pyplot.subplots(figsize=(15, 5))\n",
    "df[train_period-48:]['mean'].plot(ax=ax)\n",
    "df[train_period-48:]['predicted'].plot(ax=ax)\n",
    "# (df[train_period-48:]['mean'] - df[train_period-48:]['predicted']).plot(ax=ax)\n",
    "# ax.fill_between(df.index, df['predicted_lower'], df['predicted_upper'], color='k', alpha=0.1);  \n",
    "pyplot.legend()\n",
    "pyplot.show()\n",
    "\n",
    "daily_sums = df[train_period:].resample('D').sum()\n",
    "print(mean_absolute_error(df[train_period:]['mean'], df[train_period:]['predicted']))\n",
    "print(daily_sums)\n",
    "print(mean_squared_error(daily_sums['mean'], daily_sums['predicted']))\n",
    "print(mean_absolute_error(daily_sums['mean'], daily_sums['predicted']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
